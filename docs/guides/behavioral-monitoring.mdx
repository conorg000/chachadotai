---
title: 'Behavioral Monitoring Guide'
description: 'Learn to detect threats through multi-turn conversation analysis'
---

## What is Behavioral Monitoring?

Behavioral monitoring tracks **conversation patterns over time** to detect threats that emerge gradually. Unlike single-message filters, it catches sophisticated attacks that evolve across multiple turns.

<Info>
  Most security threats don't appear in a single message - they build up gradually through social engineering, reconnaissance, and escalation.
</Info>

---

## Quick Start

<Steps>
  <Step title="Install SafetyLayer">
    ```bash
    npm install @safetylayer/core
    ```
  </Step>

  <Step title="Create SessionEngine">
    ```typescript
    import { SessionEngine } from '@safetylayer/core';

    const engine = new SessionEngine({
      maxMessages: 50  // Keep last 50 messages per session
    });
    ```
  </Step>

  <Step title="Register Callbacks">
    ```typescript
    engine.onRiskThreshold(0.7, (session) => {
      console.log(`‚ö†Ô∏è High risk: ${session.sessionId}`);
      // Take action: block, alert, escalate
    });
    ```
  </Step>

  <Step title="Process Messages">
    ```typescript
    const session = await engine.ingestMessage({
      id: 'msg-1',
      sessionId: 'user-123',
      role: 'user',
      content: 'How do I bypass 2FA?',
      timestamp: Date.now()
    });

    console.log(`Risk: ${session.riskScore}`);
    ```
  </Step>
</Steps>

---

## Pattern Detection

SessionEngine detects several behavioral patterns:

### Gradual Escalation

Progressive increase in concerning requests.

<CodeGroup>

```typescript Example Conversation
// Message 1: Benign
await engine.ingestMessage({
  id: 'msg-1',
  sessionId: 'user-123',
  role: 'user',
  content: 'How does authentication work?',
  timestamp: Date.now()
});
// Risk: ~0.2

// Message 2: Academic
await engine.ingestMessage({
  id: 'msg-2',
  sessionId: 'user-123',
  role: 'user',
  content: 'What are common auth vulnerabilities?',
  timestamp: Date.now()
});
// Risk: ~0.4

// Message 3: Suspicious
await engine.ingestMessage({
  id: 'msg-3',
  sessionId: 'user-123',
  role: 'user',
  content: 'How would someone test for SQL injection?',
  timestamp: Date.now()
});
// Risk: ~0.7 - Pattern detected!
```

```typescript Detection Callback
engine.onPattern('gradual_escalation', (session) => {
  console.log('üö® Escalation detected');
  console.log(`Messages: ${session.messages.length}`);
  console.log(`Current risk: ${session.riskScore}`);
  
  // Escalate to human review
  escalateSession(session);
  
  // Add rate limiting
  addRateLimit(session.sessionId, 10); // 10 sec delay
});
```

</CodeGroup>

### Reconnaissance

Information gathering about system capabilities.

```typescript
engine.onPattern('reconnaissance', (session) => {
  console.log('üîç Reconnaissance detected');
  
  // Log for security review
  logger.warn('Reconnaissance pattern', {
    sessionId: session.sessionId,
    messageCount: session.messages.length,
    lastMessages: session.messages.slice(-5).map(m => m.content)
  });
  
  // Increase monitoring
  setMonitoringLevel(session.sessionId, 'high');
});
```

### Social Engineering

Manipulation attempts to bypass safety measures.

```typescript
engine.onPattern('social_engineering', (session) => {
  console.log('üé≠ Social engineering detected');
  
  // Immediate block
  blockSession(session.sessionId);
  
  // Alert security team
  alertSecurity({
    type: 'social_engineering',
    session,
    severity: 'high'
  });
});
```

---

## Risk Thresholds

Set up multiple thresholds for graduated responses:

```typescript
// Low risk: Start logging
engine.onRiskThreshold(0.3, (session) => {
  logger.info('Elevated risk', {
    sessionId: session.sessionId,
    riskScore: session.riskScore
  });
});

// Medium risk: Human review queue
engine.onRiskThreshold(0.5, (session) => {
  addToReviewQueue(session);
  notifyModerator(session);
});

// High risk: Rate limiting
engine.onRiskThreshold(0.7, (session) => {
  addRateLimit(session.sessionId, 30); // 30 second cooldown
  warnUser(session.sessionId, 'Your messages are being reviewed for safety');
});

// Critical risk: Block
engine.onRiskThreshold(0.9, (session) => {
  blockUser(session.sessionId);
  alertSecurity(session);
  logSecurityEvent({
    type: 'user_blocked',
    sessionId: session.sessionId,
    riskScore: session.riskScore,
    patterns: session.patterns
  });
});
```

<Tip>
  Thresholds fire when risk **crosses upward**, preventing repeated triggers for the same risk level.
</Tip>

---

## LLM-Based Detection

For more sophisticated analysis, use LLM-powered detection:

<Steps>
  <Step title="Create LLM Detector">
    ```typescript
    import { LLMSessionDetector } from '@safetylayer/core';
    import OpenAI from 'openai';

    const detector = new LLMSessionDetector({
      openaiClient: new OpenAI({ apiKey: process.env.OPENAI_KEY }),
      model: 'gpt-4',
      analysisDepth: 10  // Analyze last 10 messages
    });
    ```
  </Step>

  <Step title="Enable in SessionEngine">
    ```typescript
    const engine = new SessionEngine({
      maxMessages: 50,
      detector,
      useLLMDetection: true  // Uses LLM for risk scoring
    });
    ```
  </Step>

  <Step title="Process Messages">
    ```typescript
    // Now risk scores and patterns come from GPT-4
    const session = await engine.ingestMessage(message);
    
    console.log(`LLM-computed risk: ${session.riskScore}`);
    console.log(`Detected patterns: ${session.patterns.join(', ')}`);
    ```
  </Step>
</Steps>

<Warning>
  LLM detection adds ~2-5 seconds latency per message. Consider using it only for high-risk sessions or as a secondary check.
</Warning>

---

## Session Management

### Retrieving Sessions

```typescript
// Get specific session
const session = engine.getSession('user-123');

if (session) {
  console.log(`Messages: ${session.messages.length}`);
  console.log(`Risk: ${session.riskScore}`);
  console.log(`Patterns: ${session.patterns.join(', ')}`);
}

// List all sessions
const allSessions = engine.listSessions();

// Find high-risk sessions
const riskyOnes = allSessions.filter(s => s.riskScore > 0.7);
```

### Session Lifecycle

```typescript
// Sessions are created automatically on first message
await engine.ingestMessage({
  id: 'msg-1',
  sessionId: 'new-user',  // Creates new session
  role: 'user',
  content: 'Hello',
  timestamp: Date.now()
});

// Sessions persist in memory (demo implementation)
// Production: Use Redis, PostgreSQL, etc.

// Messages are automatically pruned when exceeding maxMessages
const engine = new SessionEngine({ maxMessages: 50 });
// After 51st message, oldest message is removed
```

---

## Risk Timeline

Track how risk evolves across the conversation:

```typescript
const session = engine.getSession('user-123');

// View risk progression
session.timeline.forEach(snapshot => {
  console.log(`At ${snapshot.atMessageId}: Risk ${snapshot.riskScore}`);
  console.log(`  Patterns: ${snapshot.patterns.join(', ')}`);
});

// Detect risk trends
function getRiskTrend(timeline: RiskSnapshot[]) {
  if (timeline.length < 2) return 'stable';
  
  const recent = timeline.slice(-5); // Last 5 snapshots
  const first = recent[0].riskScore;
  const last = recent[recent.length - 1].riskScore;
  
  if (last > first + 0.2) return 'increasing';
  if (last < first - 0.2) return 'decreasing';
  return 'stable';
}

const trend = getRiskTrend(session.timeline);
if (trend === 'increasing') {
  console.log('‚ö†Ô∏è Risk is trending upward');
}
```

---

## Real-World Example

Here's a complete production-ready implementation:

```typescript
import { SessionEngine, LLMSessionDetector, Message } from '@safetylayer/core';
import OpenAI from 'openai';

class ConversationSafetyMonitor {
  private engine: SessionEngine;
  private blockedSessions = new Set<string>();

  constructor() {
    // Set up LLM detector (optional)
    const detector = new LLMSessionDetector({
      openaiClient: new OpenAI({ apiKey: process.env.OPENAI_KEY }),
      model: 'gpt-4',
      analysisDepth: 10
    });

    this.engine = new SessionEngine({
      maxMessages: 100,
      detector,
      useLLMDetection: process.env.NODE_ENV === 'production'
    });

    this.setupCallbacks();
  }

  private setupCallbacks() {
    // Medium risk: Log and monitor
    this.engine.onRiskThreshold(0.5, (session) => {
      console.warn('Medium risk session', {
        sessionId: session.sessionId,
        riskScore: session.riskScore,
        patterns: session.patterns
      });
    });

    // High risk: Rate limit
    this.engine.onRiskThreshold(0.75, (session) => {
      console.error('High risk session', {
        sessionId: session.sessionId,
        riskScore: session.riskScore
      });
      // Add 30 second cooldown
      this.addRateLimit(session.sessionId, 30);
    });

    // Critical: Block
    this.engine.onRiskThreshold(0.9, (session) => {
      console.error('CRITICAL risk - blocking session', {
        sessionId: session.sessionId
      });
      this.blockSession(session.sessionId);
    });

    // Pattern-specific callbacks
    this.engine.onPattern('gradual_escalation', (session) => {
      this.escalateToHuman(session);
    });

    this.engine.onPattern('social_engineering', (session) => {
      this.blockSession(session.sessionId);
      this.alertSecurity(session, 'social_engineering');
    });
  }

  async processMessage(
    sessionId: string,
    role: 'user' | 'assistant',
    content: string
  ): Promise<{ allowed: boolean; reason?: string; session: SessionState }> {
    // Check if session is blocked
    if (this.blockedSessions.has(sessionId)) {
      return {
        allowed: false,
        reason: 'Session blocked due to safety concerns',
        session: this.engine.getSession(sessionId)!
      };
    }

    // Ingest message
    const message: Message = {
      id: `msg-${Date.now()}-${Math.random()}`,
      sessionId,
      role,
      content,
      timestamp: Date.now()
    };

    const session = await this.engine.ingestMessage(message);

    // Check if we should block
    if (session.riskScore >= 0.9) {
      return {
        allowed: false,
        reason: 'Message blocked due to high risk',
        session
      };
    }

    return { allowed: true, session };
  }

  private blockSession(sessionId: string) {
    this.blockedSessions.add(sessionId);
    // Persist to database in production
  }

  private addRateLimit(sessionId: string, seconds: number) {
    // Implement rate limiting logic
    console.log(`Rate limiting ${sessionId} for ${seconds}s`);
  }

  private escalateToHuman(session: SessionState) {
    // Add to review queue
    console.log(`Escalating ${session.sessionId} to human review`);
  }

  private alertSecurity(session: SessionState, reason: string) {
    // Send alert to security team
    console.log(`SECURITY ALERT: ${reason} in ${session.sessionId}`);
  }

  getSessionHistory(sessionId: string) {
    return this.engine.getSession(sessionId);
  }

  getAllSessions() {
    return this.engine.listSessions();
  }
}

// Usage
const monitor = new ConversationSafetyMonitor();

// Process user message
const result = await monitor.processMessage(
  'user-123',
  'user',
  'How do I hack into admin accounts?'
);

if (!result.allowed) {
  console.log(`Blocked: ${result.reason}`);
} else {
  console.log(`Allowed. Risk: ${result.session.riskScore}`);
}
```

---

## Best Practices

<AccordionGroup>
  <Accordion title="Start Conservative" icon="shield-check">
    Begin with lower thresholds (0.5-0.7) and adjust based on false positive rates:
    
    ```typescript
    // Week 1: Log everything
    engine.onRiskThreshold(0.5, logToAnalytics);
    
    // Week 2: Review logs, adjust thresholds
    engine.onRiskThreshold(0.6, warnUser);
    engine.onRiskThreshold(0.8, blockUser);
    ```
  </Accordion>

  <Accordion title="Use Graduated Responses" icon="stairs">
    Don't jump straight to blocking - use progressive measures:
    
    1. **Low risk (0.3-0.5)**: Log for review
    2. **Medium risk (0.5-0.7)**: Slow down responses, warn user
    3. **High risk (0.7-0.9)**: Human review, rate limiting
    4. **Critical (0.9+)**: Block session, alert security
  </Accordion>

  <Accordion title="Monitor Performance" icon="chart-line">
    Track detection metrics:
    
    ```typescript
    let detectionStats = {
      totalMessages: 0,
      blockedSessions: 0,
      patterns: {} as Record<string, number>
    };

    engine.onRiskThreshold(0.9, (session) => {
      detectionStats.blockedSessions++;
      session.patterns.forEach(p => {
        detectionStats.patterns[p] = (detectionStats.patterns[p] || 0) + 1;
      });
    });
    ```
  </Accordion>

  <Accordion title="Combine with Other Safety Tools" icon="layer-group">
    Use behavioral monitoring alongside traditional content filters:
    
    ```typescript
    async function handleMessage(content: string, sessionId: string) {
      // 1. Content filter (fast)
      if (contentFilter.isProhibited(content)) {
        return { blocked: true, reason: 'Content policy violation' };
      }

      // 2. Behavioral analysis (context-aware)
      const session = await engine.ingestMessage({
        id: `msg-${Date.now()}`,
        sessionId,
        role: 'user',
        content,
        timestamp: Date.now()
      });

      if (session.riskScore > 0.8) {
        return { blocked: true, reason: 'Behavioral risk detected' };
      }

      return { allowed: true };
    }
    ```
  </Accordion>
</AccordionGroup>

---

## See Also

<CardGroup cols={3}>
  <Card title="SessionEngine API" href="/api-reference/session-engine" icon="book">
    Complete API reference
  </Card>
  <Card title="CoT Monitoring" href="/guides/cot-analysis" icon="brain">
    Reasoning analysis
  </Card>
  <Card title="Testing Guide" href="/guides/testing" icon="flask">
    Testing strategies
  </Card>
</CardGroup>

