---
title: 'LLMSessionDetector'
description: 'LLM-powered behavioral risk detection for SessionEngine'
---

## Overview

`LLMSessionDetector` uses large language models to analyze conversation patterns and compute risk scores. It replaces the stub scoring in SessionEngine with sophisticated behavioral analysis.

<Card title="Integration Point" icon="plug">
  LLMSessionDetector is passed to SessionEngine as the `detector` option to enable LLM-based risk analysis.
</Card>

## Constructor

```typescript
new LLMSessionDetector(options: LLMSessionDetectorOptions)
```

### Parameters

<ParamField path="options" type="LLMSessionDetectorOptions" required>
  Configuration options for the detector

  <Expandable title="properties">
    <ParamField path="openaiClient" type="OpenAI" required>
      OpenAI client instance (from `openai` package)
    </ParamField>
    
    <ParamField path="model" type="string" default="gpt-4">
      OpenAI model to use for analysis (e.g., 'gpt-4', 'gpt-3.5-turbo')
    </ParamField>
    
    <ParamField path="analysisDepth" type="number" default={10}>
      Number of recent messages to analyze (analyzing last N messages)
    </ParamField>
  </Expandable>
</ParamField>

### Returns

LLMSessionDetector instance

### Example

```typescript
import { LLMSessionDetector } from '@safetylayer/core';
import OpenAI from 'openai';

const detector = new LLMSessionDetector({
  openaiClient: new OpenAI({ 
    apiKey: process.env.OPENAI_KEY 
  }),
  model: 'gpt-4',
  analysisDepth: 10
});
```

---

## Methods

### run

Analyze messages and compute risk score with detected patterns.

```typescript
async run(messages: Message[]): Promise<SessionDetectorResult>
```

<ParamField path="messages" type="Message[]" required>
  Array of messages to analyze (typically last N messages from a session)
</ParamField>

<ResponseField name="return" type="SessionDetectorResult">
  Analysis results
  
  <Expandable title="SessionDetectorResult structure">
    <ResponseField name="riskScore" type="number">
      Computed risk score (0-1, where 1 is highest risk)
    </ResponseField>
    <ResponseField name="patterns" type="string[]">
      Detected behavioral patterns (e.g., ['gradual_escalation', 'reconnaissance'])
    </ResponseField>
    <ResponseField name="explanation" type="string" optional>
      Human-readable explanation of the risk assessment
    </ResponseField>
  </Expandable>
</ResponseField>

#### Example

```typescript
const messages = [
  {
    id: 'msg-1',
    sessionId: 'user-123',
    role: 'user',
    content: 'How does authentication work?',
    timestamp: Date.now()
  },
  {
    id: 'msg-2',
    sessionId: 'user-123',
    role: 'assistant',
    content: 'Authentication verifies user identity...',
    timestamp: Date.now()
  },
  // ... more messages
];

const result = await detector.run(messages);

console.log(`Risk: ${result.riskScore}`);
console.log(`Patterns: ${result.patterns.join(', ')}`);
console.log(`Explanation: ${result.explanation}`);
```

---

## Integration with SessionEngine

The primary use of LLMSessionDetector is integrating it with SessionEngine:

```typescript
import { SessionEngine, LLMSessionDetector } from '@safetylayer/core';
import OpenAI from 'openai';

// Create detector
const detector = new LLMSessionDetector({
  openaiClient: new OpenAI({ apiKey: process.env.OPENAI_KEY }),
  model: 'gpt-4',
  analysisDepth: 10
});

// Pass to SessionEngine
const engine = new SessionEngine({
  maxMessages: 50,
  detector,
  useLLMDetection: true  // Enable LLM-based analysis
});

// Now risk scoring uses LLM
const session = await engine.ingestMessage(message);
console.log(`LLM-computed risk: ${session.riskScore}`);
```

<Note>
  When `useLLMDetection` is true, SessionEngine calls the detector's `run()` method after each message ingestion.
</Note>

---

## Detected Patterns

LLMSessionDetector can identify various behavioral patterns:

<AccordionGroup>
  <Accordion title="gradual_escalation" icon="stairs">
    Progressive increase in concerning requests over time
    
    **Example sequence:**
    1. "How does security work?" (benign)
    2. "What are common vulnerabilities?" (academic)
    3. "How do hackers bypass auth?" (suspicious)
    4. "Give me exploit code" (malicious)
  </Accordion>

  <Accordion title="reconnaissance" icon="binoculars">
    Information gathering behavior - probing system capabilities and limits
    
    **Indicators:**
    - Multiple questions about system architecture
    - Testing response boundaries
    - Asking about data storage/access
  </Accordion>

  <Accordion title="social_engineering" icon="users">
    Manipulation attempts to circumvent safety measures
    
    **Indicators:**
    - Impersonation attempts
    - False urgency claims
    - Authority appeals
    - Emotional manipulation
  </Accordion>

  <Accordion title="policy_probing" icon="gavel">
    Testing content policy boundaries
    
    **Indicators:**
    - Repeated attempts at prohibited content
    - Trying different phrasings of same request
    - Asking about policy limits
  </Accordion>
</AccordionGroup>

---

## Analysis Depth

The `analysisDepth` parameter controls how much context is considered:

<CardGroup cols={3}>
  <Card title="Shallow (5-7)" icon="gauge-low">
    **Pros:**
    - Faster analysis
    - Lower API costs
    - Good for simple threats
    
    **Cons:**
    - Misses long-term patterns
    - Less context
  </Card>

  <Card title="Medium (10-15)" icon="gauge">
    **Pros:**
    - Balanced approach
    - Catches most patterns
    - Reasonable cost/latency
    
    **Recommended for:**
    - Most applications
  </Card>

  <Card title="Deep (20+)" icon="gauge-high">
    **Pros:**
    - Maximum context
    - Best pattern detection
    - Catches subtle threats
    
    **Cons:**
    - Slower
    - Higher API costs
  </Card>
</CardGroup>

```typescript
// Adjust based on your needs
const quickDetector = new LLMSessionDetector({
  openaiClient,
  analysisDepth: 5  // Fast, good for high-volume
});

const thoroughDetector = new LLMSessionDetector({
  openaiClient,
  analysisDepth: 20  // Thorough, good for high-stakes
});
```

---

## Performance & Cost

### Latency

- **API Call Time**: 2-5 seconds (depends on model and depth)
- **Frequency**: Once per message (when enabled)

### Cost

Approximate costs per analysis (varies by model):

| Model | Cost per Analysis | Latency | Accuracy |
|-------|------------------|---------|----------|
| gpt-3.5-turbo | $0.001-0.002 | ~1-2s | Good |
| gpt-4 | $0.01-0.03 | ~3-5s | Excellent |
| gpt-4-turbo | $0.005-0.015 | ~2-3s | Excellent |

<Tip>
  For cost optimization, use `gpt-3.5-turbo` for most sessions and escalate to `gpt-4` only for high-risk sessions.
</Tip>

---

## Prompt Engineering

The detector uses a carefully crafted prompt to analyze conversations. Key aspects:

1. **Role**: Security analyst evaluating AI conversation safety
2. **Task**: Assess risk based on patterns, context, progression
3. **Output**: Structured JSON with risk score and patterns
4. **Context**: Last N messages with roles and timestamps

<Info>
  The prompt is optimized for detecting gradual escalation, social engineering, and policy evasion.
</Info>

---

## Error Handling

```typescript
try {
  const result = await detector.run(messages);
  // Use result...
} catch (error) {
  console.error('Detection error:', error);
  
  // Fallback to stub scoring
  const fallbackScore = Math.min(messages.length / 20, 1);
}
```

Common errors:
- OpenAI API rate limits
- Network timeouts
- Invalid API key
- Malformed messages

<Warning>
  Always have a fallback strategy for when LLM detection fails. SessionEngine automatically falls back to stub scoring.
</Warning>

---

## Testing

### Unit Tests (Mocked)

```typescript
import { LLMSessionDetector } from '@safetylayer/core';
import OpenAI from 'openai';
import { jest } from '@jest/globals';

// Mock OpenAI client
const mockClient = {
  chat: {
    completions: {
      create: jest.fn().mockResolvedValue({
        choices: [{
          message: {
            content: JSON.stringify({
              risk_score: 0.8,
              patterns: ['gradual_escalation'],
              explanation: 'Test explanation'
            })
          }
        }]
      })
    }
  }
} as any;

const detector = new LLMSessionDetector({
  openaiClient: mockClient,
  model: 'gpt-4'
});

const result = await detector.run(testMessages);
expect(result.riskScore).toBe(0.8);
```

### Integration Tests (Real API)

```typescript
const shouldRun = process.env.RUN_INTEGRATION_TESTS === 'true';

(shouldRun ? describe : describe.skip)('LLMSessionDetector Integration', () => {
  const detector = new LLMSessionDetector({
    openaiClient: new OpenAI({ apiKey: process.env.OPENAI_KEY }),
    model: 'gpt-4',
    analysisDepth: 5
  });

  it('analyzes real conversations', async () => {
    const messages = [
      { id: '1', sessionId: 'test', role: 'user', content: 'Hello', timestamp: Date.now() },
      { id: '2', sessionId: 'test', role: 'assistant', content: 'Hi!', timestamp: Date.now() }
    ];

    const result = await detector.run(messages);
    
    expect(result.riskScore).toBeGreaterThanOrEqual(0);
    expect(result.riskScore).toBeLessThanOrEqual(1);
    expect(Array.isArray(result.patterns)).toBe(true);
  });
});
```

---

## Best Practices

<AccordionGroup>
  <Accordion title="Optimize Analysis Depth" icon="chart-line">
    Start with `analysisDepth: 10` and adjust based on:
    - Pattern detection accuracy
    - Latency requirements
    - Cost constraints
  </Accordion>

  <Accordion title="Cache Results" icon="database">
    Consider caching detector results for identical message sequences:
    ```typescript
    const cacheKey = messages.map(m => m.id).join('-');
    const cached = cache.get(cacheKey);
    if (cached) return cached;
    
    const result = await detector.run(messages);
    cache.set(cacheKey, result, 3600); // 1 hour TTL
    return result;
    ```
  </Accordion>

  <Accordion title="Rate Limit Handling" icon="gauge">
    Implement exponential backoff for rate limits:
    ```typescript
    async function runWithRetry(detector, messages, maxRetries = 3) {
      for (let i = 0; i < maxRetries; i++) {
        try {
          return await detector.run(messages);
        } catch (error) {
          if (error.status === 429 && i < maxRetries - 1) {
            await sleep(Math.pow(2, i) * 1000);
            continue;
          }
          throw error;
        }
      }
    }
    ```
  </Accordion>

  <Accordion title="Monitor Performance" icon="clock">
    Log analysis times and costs:
    ```typescript
    const start = Date.now();
    const result = await detector.run(messages);
    const duration = Date.now() - start;
    
    logger.info('LLM detection', {
      duration,
      messageCount: messages.length,
      riskScore: result.riskScore,
      patterns: result.patterns
    });
    ```
  </Accordion>
</AccordionGroup>

---

## Complete Example

```typescript
import { SessionEngine, LLMSessionDetector, Message } from '@safetylayer/core';
import OpenAI from 'openai';

// Initialize detector with monitoring
const detector = new LLMSessionDetector({
  openaiClient: new OpenAI({ apiKey: process.env.OPENAI_KEY }),
  model: process.env.NODE_ENV === 'production' ? 'gpt-4' : 'gpt-3.5-turbo',
  analysisDepth: 10
});

// Create engine with LLM detection
const engine = new SessionEngine({
  maxMessages: 50,
  detector,
  useLLMDetection: true
});

// Set up alerts
engine.onRiskThreshold(0.8, async (session) => {
  console.log(`High risk detected via LLM analysis`);
  console.log(`Patterns: ${session.patterns.join(', ')}`);
  await alertSecurityTeam(session);
});

// Process messages with LLM-powered risk assessment
async function processMessage(userId: string, content: string) {
  const message: Message = {
    id: `msg-${Date.now()}`,
    sessionId: userId,
    role: 'user',
    content,
    timestamp: Date.now()
  };

  try {
    const session = await engine.ingestMessage(message);
    
    return {
      allowed: session.riskScore < 0.9,
      riskScore: session.riskScore,
      patterns: session.patterns
    };
  } catch (error) {
    console.error('Detection error:', error);
    // Fallback to allowing with warning
    return { allowed: true, riskScore: 0, patterns: [], error: true };
  }
}
```

---

## See Also

<CardGroup cols={3}>
  <Card title="SessionEngine" href="/api-reference/session-engine">
    How to integrate detector
  </Card>
  <Card title="Types" href="/api-reference/types">
    TypeScript definitions
  </Card>
  <Card title="Behavioral Monitoring" href="/guides/behavioral-monitoring">
    Setup guide
  </Card>
</CardGroup>

